{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import lil_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first load the data and process it to form the graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "data_file = 'data/recipes_with_nutritional_info_fixed_qty.json'\n",
    "with open(data_file, 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fsa_lights_per100g': {'fat': 'green',\n",
       "  'salt': 'green',\n",
       "  'saturates': 'green',\n",
       "  'sugars': 'orange'},\n",
       " 'id': '000095fc1d',\n",
       " 'ingredients': [{'text': 'yogurt, greek, plain, nonfat'},\n",
       "  {'text': 'strawberries, raw'},\n",
       "  {'text': 'cereals ready-to-eat, granola, homemade'}],\n",
       " 'instructions': [{'text': 'Layer all ingredients in a serving dish.'}],\n",
       " 'nutr_per_ingredient': [{'fat': 0.8845044000000001,\n",
       "   'nrg': 133.80964,\n",
       "   'pro': 23.110512399999998,\n",
       "   'sat': 0.26535132,\n",
       "   'sod': 81.64656,\n",
       "   'sug': 7.348190400000001},\n",
       "  {'fat': 0.46,\n",
       "   'nrg': 49.0,\n",
       "   'pro': 1.02,\n",
       "   'sat': 0.023,\n",
       "   'sod': 2.0,\n",
       "   'sug': 7.43},\n",
       "  {'fat': 7.415,\n",
       "   'nrg': 149.25,\n",
       "   'pro': 4.17,\n",
       "   'sat': 1.207,\n",
       "   'sod': 8.0,\n",
       "   'sug': 6.04}],\n",
       " 'nutr_values_per100g': {'energy': 81.12946131894766,\n",
       "  'fat': 2.140139263515891,\n",
       "  'protein': 6.914436593565536,\n",
       "  'salt': 0.05597816738985967,\n",
       "  'saturates': 0.36534716195613937,\n",
       "  'sugars': 5.08634103436144},\n",
       " 'partition': 'train',\n",
       " 'quantity': [{'text': '8'}, {'text': '1'}, {'text': '1/4'}],\n",
       " 'title': 'Yogurt Parfaits',\n",
       " 'unit': [{'text': 'ounce'}, {'text': 'cup'}, {'text': 'cup'}],\n",
       " 'url': 'http://tastykitchen.com/recipes/breakfastbrunch/yogurt-parfaits/',\n",
       " 'weight_per_ingr': [226.796, 152.0, 30.5]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print sample recipe\n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List ingredient types\n",
    "full_ingredient_names = defaultdict(dict)\n",
    "for recipe in data:\n",
    "    for ing in recipe['ingredients']:\n",
    "        if not tuple(ing['text'].split(',')) in full_ingredient_names[ing['text'].split(',')[0]]:\n",
    "            full_ingredient_names[ing['text'].split(',')[0]][tuple(ing['text'].split(','))] = 1\n",
    "        else:\n",
    "            full_ingredient_names[ing['text'].split(',')[0]][tuple(ing['text'].split(','))] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To form the list of ingredients, we look for ingredient names that are specific enough. This means that we do not want one ingredient name to be associated to too many recipes nor to too few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build trie for each ingredient class\n",
    "class Trie:\n",
    "    def __init__(self, val, d):\n",
    "        self._build_trie(val, d)\n",
    "        \n",
    "    def _build_trie(self, val, d):\n",
    "        if len(d) == 0:\n",
    "            return\n",
    "        self.val = val\n",
    "        self.num = np.sum([d[k] for k in d])\n",
    "        # Form next level\n",
    "        children = defaultdict(dict)\n",
    "        for k in d:\n",
    "            if len(k) > 1:\n",
    "                children[k[1]][k[1:]] = d[k]\n",
    "        self.next = {}\n",
    "        for k in children:\n",
    "            self.next[k] = Trie(k, children[k])\n",
    "        return self\n",
    "    \n",
    "    def relevant_ingredients(self, thresh, name, res):\n",
    "        name.append(self.val)\n",
    "        if len(self.next) <= 1:\n",
    "            res.append((name, self.num))\n",
    "            return\n",
    "        # If all of the children have value below thresh, stop\n",
    "        children_num = [self.next[child].num for child in self.next]\n",
    "        if np.max(children_num) <= thresh:\n",
    "            res.append((name, self.num))\n",
    "            return\n",
    "        for child in self.next:\n",
    "            new_name = name.copy()\n",
    "            self.next[child].relevant_ingredients(thresh, new_name, res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the list of ingredients\n",
    "thresh = len(data) / 10\n",
    "ingredients_list = []\n",
    "for ing in full_ingredient_names:\n",
    "    t = Trie(ing, full_ingredient_names[ing])\n",
    "    res = []\n",
    "    t.relevant_ingredients(thresh, [], res)\n",
    "    ingredients_list.extend(res)\n",
    "ingredients = {''.join(ing[0]):ing[1] for ing in ingredients_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When associating ingredients to the recipes, the ones that appear in more than 10% of the recipes are considered common ingredients. These will not be taken into account when constructing the graph, but will be kept as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate ids to recipes\n",
    "recipe_frequency_thresh = len(data) / 10\n",
    "recipe_names = [recipe['title'] for recipe in data]\n",
    "recipe_ingredients = []\n",
    "for recipe in data:\n",
    "    ings_list = [ing['text'].split(',') for ing in recipe['ingredients']]\n",
    "    ings = []\n",
    "    for ing in ings_list:\n",
    "        ing_name = ing[0]\n",
    "        for i in range(1, len(ing) + 1):\n",
    "            if ing_name in ingredients:\n",
    "                if ingredients[ing_name] <= recipe_frequency_thresh:\n",
    "                    ings.append(ing_name)\n",
    "                break\n",
    "            ing_name += ing[i]\n",
    "    recipe_ingredients.append(ings)\n",
    "recipe_df = pd.DataFrame({'name': recipe_names, 'ingredients': recipe_ingredients})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group recipes by ingredients\n",
    "recipes_by_ingredients = defaultdict(list)\n",
    "for ind in recipe_df.index:\n",
    "    ings = recipe_df.iloc[ind]['ingredients']\n",
    "    for ing in ings:\n",
    "        recipes_by_ingredients[ing].append(ind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph generation will be done in C++ due to memory and time constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write recipes by ingredients as string\n",
    "str_data = f'{len(data)} {len(recipes_by_ingredients)}\\n'\n",
    "for ing in recipes_by_ingredients:\n",
    "    str_data += f\"{len(recipes_by_ingredients[ing])}\\n{' '.join([str(r) for r in recipes_by_ingredients[ing]])}\\n\"\n",
    "\n",
    "with open('data/recipes_by_ingredients.txt', 'w') as f:\n",
    "    f.write(str_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtained a adjacency lists which we would like to convert to a sparse matrix. However, the amount of data is very large (text file of 2G of edges) and it may not fit into memory. We therefore need to perform some sampling of the edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "recipe_graph_file = 'data/recipe_graph.txt'\n",
    "\n",
    "edge_values = []\n",
    "with open(recipe_graph_file, 'r') as f:\n",
    "    for i in range(len(data)):\n",
    "        edges = f.readline().split(' ')[1:-1]\n",
    "        edges_val = [float(edge[1:-1].split(',')[1]) for edge in edges]\n",
    "        edge_values += edges_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of edges is 126549982.\n",
      "The smallest edge value is 0.151142.\n",
      "The largest edge value is 15.6836.\n",
      "The 90th percentile of the edge values is 1.64314.\n"
     ]
    }
   ],
   "source": [
    "perc10 = np.percentile(edge_values, 90)\n",
    "print(f'The number of edges is {len(edge_values)}.')\n",
    "print(f'The smallest edge value is {np.min(edge_values)}.')\n",
    "print(f'The largest edge value is {np.max(edge_values)}.')\n",
    "print(f'The 90th percentile of the edge values is {perc10}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's then sample the 10% most significant edges."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
